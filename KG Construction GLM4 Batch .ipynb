{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78da0fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 results already exist. Loading them...\n",
      "Stage 2 results do not exist or forced to run from scratch. Running Stage 2...\n",
      "File uploaded successfully. File ID: 1740673288_7d16501679594b02a9ed2188014d9da2\n",
      "Batch job created successfully. Batch ID: batch_1895147252189110272\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: validating\n",
      "Current status: in_progress\n",
      "Current status: in_progress\n",
      "Current status: in_progress\n",
      "Current status: completed\n",
      "Batch processing completed. Downloading results.\n",
      "Results downloaded and saved to KG/batch_output_stage_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from zhipuai import ZhipuAI\n",
    "import requests\n",
    "\n",
    "# Initialize ZhipuAI client\n",
    "client = ZhipuAI(api_key=\"_____\")  # Replace with your actual API key\n",
    "\n",
    "# Folder paths and files\n",
    "disease_folder = \"disease introduction/filtered\"  # Folder containing filtered files\n",
    "output_path = \"KG/\"  # Output folder for processed files\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Function to generate the OIE prompt\n",
    "def get_oie_prompt(disease_name, disease_intro):\n",
    "    return f\"\"\"\n",
    "    Given the disease name as '{disease_name}', extract the relevant entities and relationships from the following disease introduction:\n",
    "    {disease_intro}.\n",
    "    \n",
    "    Instructions:\n",
    "    1. The **symptoms**, **emergency examination**, **precautions**, **prognosis**, and other such terms should be treated as **relations**, not as subjects or objects.\n",
    "    2. Extract **entities** as specific **noun phrases** or **proper nouns**, avoiding full sentences or long descriptions. For example, for the symptom 'fever and headache', use 'fever' and 'headache' as entities, rather than the full sentence.\n",
    "    3. Do **not combine** multiple pieces of information into a single triplet. If a subject and object contain multiple related pieces of information, they should be split into separate triplets. For example, if a symptom includes multiple manifestations (e.g., 'fatigue, chest pain'), each manifestation should be captured as an individual triplet.\n",
    "    4. Ensure that the knowledge graph is **connected**, meaning that the subject and object are linked through meaningful relationships, and that no entities or relations are isolated or fragmented.\n",
    "    5. Avoid confusing relations and entities. Relations should represent connections between entities (e.g., 'causes', 'leads to', 'is treated with'), whereas entities should represent specific items such as disease names, symptoms, treatments, etc.\n",
    "    6. When extracting relationships, ensure clarity and avoid ambiguity. If the sentence mentions multiple relationships or entities, separate them clearly into distinct triplets.\n",
    "\n",
    "    Return the extracted relationships as a list of triplets in the form of [Subject, Relation, Object].\n",
    "    Example: ['Thyroid cancer', 'isTypeOf', 'Malignant tumor'].\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Function to generate the relation definition prompt\n",
    "def get_relation_definition_prompt(disease_name, KG, relations):\n",
    "    return f\"\"\"\n",
    "    Given the extracted relationships from the disease '{disease_name}', define each relation in clear and natural language.\n",
    "    The original knowledge graph (KG) is shown below: '{KG}'.\n",
    "    \n",
    "    Instructions:\n",
    "    1. Your definitions should be **general** and **applicable** across different diseases, ensuring that they can be used in multiple disease knowledge graphs.\n",
    "    2. Do not include any **disease-specific information** (e.g., symptoms, treatments, or specific diseases) in the relationship definitions.\n",
    "    3. The definitions should be **simple, precise, and concise**, focusing on the relationship itself without detailing specific examples.\n",
    "    \n",
    "    Example:\n",
    "    - 'isTypeOf': 'This relation indicates that the subject is a category or classification of the object entity, typically categorizing an entity in terms of a broader class.'\n",
    "    \n",
    "    Now, please provide definitions for the following relations:\n",
    "    {relations}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Function to generate the canonicalization prompt\n",
    "def get_canonicalization_prompt(disease_name, relations, definitions):\n",
    "    return f\"\"\"\n",
    "    Given the relationships for disease '{disease_name}', standardize the following relations to ensure consistency across diseases:\n",
    "    {relations}\n",
    "    Use the definitions provided below to ensure the relationships are standardized:\n",
    "    {definitions}\n",
    "    \"\"\"\n",
    "\n",
    "# Helper function to upload JSONL to API and start batch job\n",
    "def upload_jsonl_and_create_batch(jsonl_path):\n",
    "    with open(jsonl_path, \"rb\") as f:\n",
    "        result = client.files.create(file=f, purpose=\"batch\")\n",
    "        print(f\"File uploaded successfully. File ID: {result.id}\")\n",
    "        batch_job = client.batches.create(\n",
    "            input_file_id=result.id,\n",
    "            endpoint=\"/v4/chat/completions\",\n",
    "            auto_delete_input_file=True,\n",
    "            metadata={\"description\": \"Disease Knowledge Graph Construction\"}\n",
    "        )\n",
    "        print(f\"Batch job created successfully. Batch ID: {batch_job.id}\")\n",
    "        return batch_job.id\n",
    "\n",
    "# Poll for batch processing status\n",
    "def check_batch_status(batch_id):\n",
    "    while True:\n",
    "        batch_status = client.batches.retrieve(batch_id)\n",
    "        print(f\"Current status: {batch_status.status}\")\n",
    "        if batch_status.status == \"completed\":\n",
    "            print(\"Batch processing completed. Downloading results.\")\n",
    "            return True\n",
    "        elif batch_status.status in [\"failed\", \"expired\", \"cancelled\"]:\n",
    "            print(\"Batch processing failed or cancelled.\")\n",
    "            return False\n",
    "        time.sleep(30)\n",
    "\n",
    "# Download the batch processing results\n",
    "def download_results(batch_id, stage):\n",
    "    batch_job = client.batches.retrieve(batch_id)\n",
    "    result_file_id = batch_job.output_file_id\n",
    "    content = client.files.content(result_file_id)\n",
    "    result_file = os.path.join(output_path, f\"batch_output_stage_{stage}.jsonl\")\n",
    "    content.write_to_file(result_file)\n",
    "    print(f\"Results downloaded and saved to {result_file}\")\n",
    "    return result_file\n",
    "\n",
    "# Process the results from the API and save\n",
    "def process_results(result_file, stage):\n",
    "    df = pd.read_json(result_file, lines=True)\n",
    "    df[f'KG_{stage}'] = df[\"response\"].apply(lambda x: x['body']['choices'][0]['message']['content'].strip())\n",
    "    output_file = os.path.join(output_path, f'processed_results_{stage}.json')\n",
    "    df.to_json(output_file, orient=\"records\", lines=True, force_ascii=False)\n",
    "    return df\n",
    "\n",
    "# Helper function to read all disease files and gather text data\n",
    "def load_disease_data_from_files():\n",
    "    disease_data = {}\n",
    "    for filename in os.listdir(disease_folder):\n",
    "        if filename.endswith(\".txt\"):  # Process only txt files\n",
    "            disease_name = filename.replace('.txt', '')\n",
    "            file_path = os.path.join(disease_folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                disease_intro = file.read()\n",
    "                disease_data[disease_name] = disease_intro\n",
    "    return disease_data\n",
    "\n",
    "\n",
    "def process_kg_1_relation(kg_1_str):\n",
    "    # Split the KG_1 string by each line (using '\\n' to split the data)\n",
    "    kg_1_lines = kg_1_str.strip().split('\\n')\n",
    "    kg_1_relations = []\n",
    "    \n",
    "    for line in kg_1_lines:\n",
    "        # Remove the line number and square brackets\n",
    "        line = line.strip()\n",
    "        line = line.split(\"[\")[1] if \"[\" in line else line  # Remove the numeric prefix (e.g., \"37. \")\n",
    "#         print(line)\n",
    "#         line = line[1]\n",
    "        parts = line.strip().strip('[]').split(\"', '\")\n",
    "        \n",
    "        if len(parts) == 3:\n",
    "            # Remove single quotes and store the triplet\n",
    "            relation = parts[1].strip(\"'\")\n",
    "            kg_1_relations.append(relation)\n",
    "    kg_1_relations = set(kg_1_relations)\n",
    "    return kg_1_relations\n",
    "\n",
    "\n",
    "# Helper function to process each stage\n",
    "def process_stage(disease_data, stage, previous_stage_results=None, force_run=False):\n",
    "    all_requests = []\n",
    "    for disease_name, disease_intro in disease_data.items():\n",
    "        custom_id = disease_name if len(disease_name) >= 6 else disease_name.ljust(6, \"_\")\n",
    "\n",
    "        # Stage 1: Information extraction (OIE)\n",
    "        if stage == 1:\n",
    "            prompt = get_oie_prompt(disease_name, disease_intro)\n",
    "#             print(f\"Stage 1 Prompt: {prompt}\")  # Debug log for Stage 1 prompt\n",
    "\n",
    "        # Stage 2: Relation definition (using Stage 1 results)\n",
    "        elif stage == 2:\n",
    "            if not force_run and previous_stage_results is not None:\n",
    "                # Use Stage 1 output (relations) to define relations\n",
    "                KG_ONE = previous_stage_results.loc[custom_id][\"KG_1\"]\n",
    "                relations = process_kg_1_relation(KG_ONE)\n",
    "                \n",
    "                prompt = get_relation_definition_prompt(disease_name, KG_ONE, relations)\n",
    "#                 print(f\"Stage 2 Prompt: {prompt}\")  # Debug log for Stage 2 prompt\n",
    "            else:\n",
    "                prompt = get_relation_definition_prompt(disease_name, [], [])\n",
    "\n",
    "        # Stage 3: Canonicalization (using Stage 1 and Stage 2 outputs)\n",
    "        elif stage == 3:\n",
    "            if not force_run and previous_stage_results is not None:\n",
    "                # Use Stage 2 output (definitions) and Stage 1 relations for standardization\n",
    "                relations = previous_stage_results.loc[custom_id][\"KG_1\"]\n",
    "                definitions = previous_stage_results.loc[custom_id][\"KG_2\"]\n",
    "                prompt = get_canonicalization_prompt(disease_name, relations, definitions)\n",
    "#                 print(f\"Stage 3 Prompt: {prompt}\")  # Debug log for Stage 3 prompt\n",
    "            else:\n",
    "                prompt = get_canonicalization_prompt(disease_name, [], [])\n",
    "\n",
    "        request = {\n",
    "            \"custom_id\": custom_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v4/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"glm-4-flash\",  # Adjust this model if necessary\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": f\"Process disease information for '{disease_name}'.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        all_requests.append(json.dumps(request, ensure_ascii=False))\n",
    "\n",
    "    jsonl_path = f\"KG/batch_requests_KG_stage_{stage}.jsonl\"  # Path to save the JSONL file\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(all_requests))\n",
    "\n",
    "    batch_id = upload_jsonl_and_create_batch(jsonl_path)\n",
    "    if check_batch_status(batch_id):\n",
    "        result_file = download_results(batch_id, stage)\n",
    "        return process_results(result_file, stage)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Main entry point for running the entire process\n",
    "def main(force_run_all=False):\n",
    "    # Load disease data from filtered folder\n",
    "    disease_data = load_disease_data_from_files()\n",
    "\n",
    "    # Stage 1: Information extraction (OIE)\n",
    "    stage_1_file = os.path.join(output_path, \"processed_results_1.json\")\n",
    "    if os.path.exists(stage_1_file) and not force_run_all:\n",
    "        print(\"Stage 1 results already exist. Loading them...\")\n",
    "        output_file = os.path.join(output_path, f'processed_results_1.json')\n",
    "        stage_1_results = pd.read_json(output_file, lines=True)\n",
    "        stage_1_results = stage_1_results.set_index('custom_id')\n",
    "#         print(stage_1_results.loc['bone tumor']['KG_1'])\n",
    "#         stage_1_results = process_results(stage_1_file, 1)\n",
    "    else:\n",
    "        print(\"Stage 1 results do not exist or forced to run from scratch. Running Stage 1...\")\n",
    "        stage_1_results = process_stage(disease_data, 1, force_run=force_run_all)\n",
    "\n",
    "    # Stage 2: Relation definition, using the output from Stage 1\n",
    "    stage_2_file = os.path.join(output_path, \"processed_results_2.json\")\n",
    "    if os.path.exists(stage_2_file) and not force_run_all:\n",
    "        print(\"Stage 2 results already exist. Loading them...\")\n",
    "#         stage_2_results = pd.read_json(stage_2_file, lines=True)\n",
    "        stage_2_results = process_results(stage_2_file, 2)\n",
    "    else:\n",
    "        print(\"Stage 2 results do not exist or forced to run from scratch. Running Stage 2...\")\n",
    "        stage_2_results = process_stage(disease_data, 2, previous_stage_results=stage_1_results, force_run=force_run_all)\n",
    "\n",
    "#     # Stage 3: Canonicalization, using the output from Stage 2\n",
    "#     stage_3_file = os.path.join(output_path, \"processed_results_3.json\")\n",
    "#     if os.path.exists(stage_3_file) and not force_run_all:\n",
    "#         print(\"Stage 3 results already exist. Loading them...\")\n",
    "#         stage_3_results = pd.read_json(stage_3_file, lines=True)\n",
    "#     else:\n",
    "#         print(\"Stage 3 results do not exist or forced to run from scratch. Running Stage 3...\")\n",
    "#         stage_3_results = process_stage(disease_data, 3, previous_stage_results=stage_2_results, force_run=force_run_all)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(force_run_all=False)  # Set to True to force re-run all stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f278574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3190be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d894a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
